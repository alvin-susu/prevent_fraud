{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 二次调优\n",
    "\n",
    "整个训练过程中,基本可以分为几步\n",
    "1. 数据加载\n",
    "2. 数据预处理\n",
    "3. 模型加载\n",
    "4. 定义LoRA参数\n",
    "5. 插入微调矩阵\n",
    "6. 定义训练参数\n",
    "7. 构建训练器开始训练\n",
    "\n",
    "这些流程基本是固定的,而训练调优过程中需要调整的是以下这些项:\n",
    "1. 输入和输出: 数据路径, 模型路径, 输出路径\n",
    "2. 参数: lora参数, 训练参数\n",
    "\n",
    "因此,我们将整个训练过程中基本不变的部分封装到train.py\n",
    "\n",
    "# 初始化\n",
    "\n",
    "使用jupyter中的魔法指令 `%run` 来嵌入一个python脚本到当前的notebook"
   ],
   "id": "2affa9ccfd67f9fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "%run train.py",
   "id": "83eb46c33847ade",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from peft import get_peft_model\n",
    "from transformers import Trainer, DataCollatorForTokenClassification, EarlyStoppingCallback\n",
    "from train import load_model, build_lora_config, build_train_arguments\n",
    "import os\n",
    "import torch"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_data_path = './datasets/train_test/test0819.jsonl'\n",
    "eval_data_path = './datasets/train_test/eval0819.jsonl'\n",
    "model_path = r\"D:\\Pretrained_models\\Qwen\\Qwen2-1___5B-Instruct\"\n",
    "output_path = './Qwen2-1___5B-Instruct_ft_0819_1'"
   ],
   "id": "1a50eb4de8d12f74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ],
   "id": "b3436705a93caa85",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "加载模型和数据集",
   "id": "5f1d7ac3a234be18"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model, tokenizer =  load_model(model_path, device=device)\n",
    "model, tokenizer"
   ],
   "id": "2d4c5f12efc87cf2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 调优-1\n",
    "每次模型在尚未训练完所有数据时就提前结束, 可能与提前结束的配置有关, 所以先从提前结束开始调整.\n",
    "- 调整点: 在构建训练器时暂时先去掉提前结束的配置,让模型跑完预设的3个epoch\n",
    "- 目的: 让数据被充分的训练,避免因损失陷入局部最小值而提前结束\n",
    "> 模型训练在验证损失还没有完全收敛时就提前停止的现象被成为Premature Early Stopping, 往往是因为验证损失存在短期波动, 而这个波动的成都不同的模型和数据集都不相同,一般需要观察验证损失在更长时间内的变化趋势,来合理的设置early_stopping_patience"
   ],
   "id": "82ff707e9bdd5d7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def build_trainer(model, tokenizer, train_args, lora_config, train_dataset, eval_dataset):\n",
    "    peft_model = get_peft_model(model, lora_config)\n",
    "    peft_model.print_trainable_parameters()\n",
    "    return Trainer(\n",
    "        model=peft_model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=DataCollatorForTokenClassification(tokenizer=tokenizer, padding=True),\n",
    "        # callbacks=[EarlyStoppingCallback(early_stopping_patience=3)] #早停回调\n",
    "    )"
   ],
   "id": "ff4678bc03359f88",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "lora参数, 训练参数都先不做调整,直接复用上次的值",
   "id": "d5d16302519f9a36"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from train import load_dataset\n",
    "\n",
    "lora_config = build_lora_config()\n",
    "train_args = build_train_arguments(output_path)\n",
    "train_dataset, eval_dataset = load_dataset(train_data_path, eval_data_path, tokenizer)\n",
    "trainer = build_trainer(model, tokenizer, train_args, lora_config, train_dataset, eval_dataset)\n",
    "trainer.train()"
   ],
   "id": "8073bb48f566a399",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "小结: 在训练期间,喂给模型的数据越多,模型学到的信息就越多,让所有数据被充分的训练时前期提高模型性能的最直接途径",
   "id": "193e9cfabf9d4615"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 调优2\n",
    "到现在为止, 小批量大小batch_size一直使用的是默认值4, 业界的经验是: `较大的batch_size有助于让梯度下降更稳定`\n",
    "\n",
    "训练参数中,我们将batch_size调整为16, 并把梯度累计由4降到1, 总的梯度下降的batch_size其实没有改变,因为 `16x1 = 4x4` 正好借此对比两种参数设置的效果\n",
    "\n",
    "> 注:batch_size 很消耗 gpu显存,需要找到适合自己GPU的尽可能大的值,一般是从一个小值开始,在没有报OOM的前提下逐步增大"
   ],
   "id": "42e8cefda83ae8f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_args = build_train_arguments(output_path)\n",
    "train_args.per_device_train_batch_size=16\n",
    "train_args.gradient_accumulation_steps=1"
   ],
   "id": "39972b31e56e1936",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "lora配置保持不变, 构建训练器开始训练",
   "id": "92bf2b341a45f5c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lora_config = build_lora_config()\n",
    "train_dataset, eval_dataset = load_dataset(train_data_path, eval_data_path, tokenizer)\n",
    "trainer = build_trainer(model, tokenizer, train_args, lora_config, train_dataset, eval_dataset)\n",
    "trainer.train()"
   ],
   "id": "b5d2dcf9e976417d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "执行评估测试",
   "id": "49e83759526ada69"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from evaluate import evaluate\n",
    "%run evaluate.py\n",
    "evaluate(model, tokenizer, lora_config, train_dataset, eval_dataset)"
   ],
   "id": "258c3eded963d15e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "训练小结: 梯度下降中,大的batch_size 比 小的batch_size 效果要好,总的batch_size相同的情况下,不带梯度累计要比使用梯度累积的效果好",
   "id": "d779427c6409a7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 调优3\n",
    "在上述损失的变化数据中,可以发现: 训练后期当训练损失不断下降时, 验证损失是没有再下降,反而时有明显的上升,这至少说明可能存在两个问题\n",
    "1. 模型再训练集上产生了过拟合, 训练损失在后期下降的太快\n",
    "2. 模型在验证集上的泛化能力有限\n",
    "\n",
    "chatgpt给出如下建议:\n",
    "1. 增加模型的正则化力度, 对于当前lora微调场景来说,也就是lora_dropout\n",
    "2. 引入学习率调度器,让学习率动态调整,特别是在训练后期,它能够让学习率逐渐减小,有助于缓解训练损失的过拟合\n",
    "> 注: 使用lora进行微调时, 原始基座模型的参数是冻结不变的,那dropout操作就只能在插入的低秩矩阵上进行, 也就是lora_dropout\n",
    "\n",
    "按照这个建议:\n",
    "1. lr_scheduler_type: 学习率调度器,这里使用余弦调度器cosine,它能够在训练过程中逐渐减小学习率 有助于模型稳定\n",
    "2. warmup_ratio: 学习率预热比例,0.05表示5%的steps用于预热,针对的是前面损失波动大的问题\n",
    "3. 将log_level调整为info以输出一些基本的日志"
   ],
   "id": "8e2b3a3639cad6cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "output_path = './Qwen2-1___5B-Instruct_ft_0213_15'\n",
    "train_args = build_train_arguments(output_path)\n",
    "train_args.per_device_train_batch_size=16\n",
    "train_args.gradient_accumulation_steps=1\n",
    "train_args.warmup_steps=0.05\n",
    "train_args.lr_scheduler_type = \"cosine\"\n",
    "train_args.log_level=\"info\""
   ],
   "id": "9d54ee233e84dd7f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "lora参数调整: 将lora_dropout从0.05增加到0.2,提高模型训练过程中的泛化能力",
   "id": "cc44ceba9e5a0610"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lora_config = build_lora_config()\n",
    "lora_config.lora_dropout=0.2"
   ],
   "id": "e09026d70b0b940",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "构建训练器开始训练",
   "id": "98eb3ee9996ece93"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "trainer = build_trainer(model, tokenizer, train_args, lora_config, train_dataset, eval_dataset)\n",
    "trainer.train()"
   ],
   "id": "8875530cdb880528"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "网上有一种说法是:drop会带来模型容量的减少,可能需要同步增加lora低秩矩阵的秩r大小,以补偿模型容量的缩减\n",
    "> 注: 同时调整学习率调度器和dropout并不好,无法区分两者带来的影响. 之调整lora_dropout起到了负面作用,只调整学习率调度器会起到正向作用"
   ],
   "id": "5307a1cda3832d4d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 调优4\n",
    "训练参数不变\n",
    "lora参数部分,dropout=0.2不变, 将微调矩阵的秩r=8调整为r=16, 缩放银子lora_alpha也保持2倍的比例增加到32"
   ],
   "id": "9a95705dbe867c2c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T07:28:01.927188Z",
     "start_time": "2025-02-13T07:28:01.916188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_path = './Qwen2-1___5B-Instruct_ft_0213_15_28'\n",
    "train_args = build_train_arguments(output_path)\n",
    "train_args.per_device_train_batch_size=16\n",
    "train_args.gradient_accumulation_steps=1\n",
    "train_args.warmup_steps=0.05\n",
    "train_args.lr_scheduler_type = \"cosine\""
   ],
   "id": "d35cc4928208e931",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T07:29:11.482152Z",
     "start_time": "2025-02-13T07:29:11.314955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lora_config = build_lora_config()\n",
    "lora_config.lora_dropout=0.2\n",
    "lora_config.r=16\n",
    "lora_config.lora_alpha=32"
   ],
   "id": "8dded754583e36dc",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'build_lora_config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m lora_config \u001B[38;5;241m=\u001B[39m \u001B[43mbuild_lora_config\u001B[49m()\n\u001B[0;32m      2\u001B[0m lora_config\u001B[38;5;241m.\u001B[39mlora_dropout\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m\n\u001B[0;32m      3\u001B[0m lora_config\u001B[38;5;241m.\u001B[39mr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m16\u001B[39m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'build_lora_config' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "trainer = build_trainer(model, tokenizer, train_args, lora_config, train_dataset, eval_dataset)\n",
    "trainer.train()"
   ],
   "id": "91adeb517a36d8e9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "训练小结: dropout需要和秩的大小配合调整, 增加秩的大小能够让模型学习到更多的参数,配合dropout一起调整能够提高模型的泛化能力",
   "id": "611e8f923c6d5ec0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
