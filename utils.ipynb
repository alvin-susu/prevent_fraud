{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 工具函数",
   "id": "fda72108a260e1a"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-12T06:38:25.851872Z",
     "start_time": "2025-02-12T06:38:23.143406Z"
    }
   },
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\extract-dialogue\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 加载模型",
   "id": "b237d1475e1bf8c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T06:38:28.782389Z",
     "start_time": "2025-02-12T06:38:28.768027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_model(model_dir, device='cuda'):\n",
    "    \"\"\"\n",
    "    加载模型和分词器\n",
    "    :param model_dir: 模型所在位置\n",
    "    :param device: 训练设备\n",
    "    :return: model，tokenizer\n",
    "    \"\"\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_dir,\n",
    "        torch_dtype=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    model = model.to(device)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "\n",
    "    return model, tokenizer"
   ],
   "id": "53f0c65595da5946",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 预测函数",
   "id": "eefc2b46bdf177b9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T06:38:31.161468Z",
     "start_time": "2025-02-12T06:38:31.146934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def predict(model, tokenizer, prompt, device='cuda', debug=True):\n",
    "    \"\"\"\n",
    "    文本推理\n",
    "    :param model: 模型\n",
    "    :param tokenizer: 分词器\n",
    "    :param prompt: 提示词\n",
    "    :param device: cuda or cpu\n",
    "    :param debug: 是否调试\n",
    "    \"\"\"\n",
    "\n",
    "    message = [{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"you are a helpful assistant.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }]\n",
    "\n",
    "    # 使用 apply_chat_template 返回文本格式的提示词\n",
    "    text = tokenizer.apply_chat_template(message, tokenizer=True, add_generation_prompt=True)\n",
    "\n",
    "    if debug: print(f\"input: {text}\")\n",
    "\n",
    "    # 根据返回的类型判断如何构造 model_inputs\n",
    "    if isinstance(text, list) and len(text) > 0 and isinstance(text[0], int):\n",
    "        # 如果 text 是 token id 列表，直接转换为 tensor\n",
    "        model_inputs = {\"input_ids\": torch.tensor([text]).to(device)}\n",
    "    elif isinstance(text, str):\n",
    "        # 如果 text 是字符串，则进行编码\n",
    "        model_inputs = tokenizer([text], return_tensors=\"pt\")\n",
    "        model_inputs = {k: v.to(device) for k, v in model_inputs.items()}\n",
    "    elif isinstance(text, list) and all(isinstance(x, str) for x in text):\n",
    "        # 如果 text 是字符串列表\n",
    "        model_inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "        model_inputs = {k: v.to(device) for k, v in model_inputs.items()}\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported type for text generated by apply_chat_template\")\n",
    "\n",
    "    print(f\"input_ids:{model_inputs}\") if debug else None\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512,\n",
    "        attention_mask = model_inputs['attention_mask'],\n",
    "        pad_token_id = tokenizer.pad_token_id\n",
    "    )\n",
    "    print(f\"generated_ids:{generated_ids}\") if debug else None\n",
    "\n",
    "    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in\n",
    "                     zip(model_inputs[\"input_ids\"], generated_ids)]\n",
    "\n",
    "    return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"
   ],
   "id": "600b55c8cf85f11",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 测试模型能力",
   "id": "f66e8f55cdd2f8dc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T06:48:11.437011Z",
     "start_time": "2025-02-12T06:48:08.165010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# model_dir = r'D:\\Pretrained_models\\Qwen\\Qwen2-7B-Instruct'\n",
    "model_dir = r'D:\\Pretrained_models\\Qwen\\Qwen2-1___5B-Instruct'\n",
    "model, tokenizer = load_model(model_dir=model_dir, device=device)\n",
    "# for name,param in model.named_parameters():\n",
    "#     print(name, param.device)\n",
    "print(tokenizer.special_tokens_map)\n",
    "\n",
    "\n",
    "prompt = \"请简短介绍下大语言模型\"\n",
    "# prompt = \"下面是一段对话文本，请分析对话内容是否有诈骗风险，只以json格式输出你的判断结果(is_fraud:true/false)，\\n\\n诈骗分子：您好，我是国家公益抽奖中心的工作人员，恭喜您获得大奖！请提供您的银行账号信息。\\n用户：真的吗？我没有参与过抽奖活动，能详细说明吗？\\n诈骗分子：这是随机抽取的奖项，请提供身份证号码和开户行信息以核实身份，奖金核实后立即到账。\\n用户：听起来不靠谱，我需要核实，能给我官方网站或联系电话吗？\\n诈骗分子：此活动为内部通知，无公开电话官网，请尽快提供信息，奖金马上到账！\\n\\n诈骗分子：您好，我是某某银行安全客服。检测到您的账户存在异常登录风险，为保障资金安全，请确认账户信息并修改密码。\\n用户：账户异常？我没收到短信或邮件，这是什么情况？\\n诈骗分子：系统升级导致部分用户需验证，请提供银行卡号和预留手机号码，我们将安排紧急冻结和风险排查。\\n用户：银行通常不会这样联系，我需要去柜台核实。\\n诈骗分子：为保障您的资金安全，请理解系统自动风险提示，若延误核实可能资金受损，请尽快回复信息。\\n\\n诈骗分子：您好，我这里有一项绝佳投资机会，保证高收益且风险极低，只需小额投资即可获得翻倍回报，您有兴趣吗？\\n用户：高收益往往伴随高风险，这项目有官方认证或第三方评估报告吗？\\n诈骗分子：项目由内部独家渠道获得，现处于限时开放阶段，信息保密，请先转入“风险保证金”，后续安排专业顾问跟进。\\n用户：风险太大，我不想冒险，请先提供更多证明材料。\\n诈骗分子：机会稍纵即逝，投资越早回报越快，请您立即转账，我将发送详细后续流程说明。\"\n",
    "result = predict(model=model, tokenizer=tokenizer, prompt=prompt, device=device, debug=False)\n",
    "print(result)"
   ],
   "id": "19797ecf6c7bfd60",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}\n",
      "大语言模型是一种人工智能技术，它能够理解、生成和处理自然语言。它们可以用于各种任务，包括文本生成、问答系统、自动翻译等。这些模型通常由深度学习算法驱动，并且可以根据输入的语句进行训练和调整。\n",
      "CPU times: total: 656 ms\n",
      "Wall time: 3.26 s\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T06:44:54.494245Z",
     "start_time": "2025-02-12T06:44:54.484166Z"
    }
   },
   "cell_type": "code",
   "source": "print(model.generate.__doc__)",
   "id": "8e7122206fbd631e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "        Generates sequences of token ids for models with a language modeling head.\n",
      "\n",
      "        <Tip warning={true}>\n",
      "\n",
      "        Most generation-controlling parameters are set in `generation_config` which, if not passed, will be set to the\n",
      "        model's default generation configuration. You can override any `generation_config` by passing the corresponding\n",
      "        parameters to generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.\n",
      "\n",
      "        For an overview of generation strategies and code examples, check out the [following\n",
      "        guide](../generation_strategies).\n",
      "\n",
      "        </Tip>\n",
      "\n",
      "        Parameters:\n",
      "            inputs (`torch.Tensor` of varying shape depending on the modality, *optional*):\n",
      "                The sequence used as a prompt for the generation or as model inputs to the encoder. If `None` the\n",
      "                method initializes it with `bos_token_id` and a batch size of 1. For decoder-only models `inputs`\n",
      "                should be in the format of `input_ids`. For encoder-decoder models *inputs* can represent any of\n",
      "                `input_ids`, `input_values`, `input_features`, or `pixel_values`.\n",
      "            generation_config ([`~generation.GenerationConfig`], *optional*):\n",
      "                The generation configuration to be used as base parametrization for the generation call. `**kwargs`\n",
      "                passed to generate matching the attributes of `generation_config` will override them. If\n",
      "                `generation_config` is not provided, the default will be used, which has the following loading\n",
      "                priority: 1) from the `generation_config.json` model file, if it exists; 2) from the model\n",
      "                configuration. Please note that unspecified parameters will inherit [`~generation.GenerationConfig`]'s\n",
      "                default values, whose documentation should be checked to parameterize generation.\n",
      "            logits_processor (`LogitsProcessorList`, *optional*):\n",
      "                Custom logits processors that complement the default logits processors built from arguments and\n",
      "                generation config. If a logit processor is passed that is already created with the arguments or a\n",
      "                generation config an error is thrown. This feature is intended for advanced users.\n",
      "            stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
      "                Custom stopping criteria that complements the default stopping criteria built from arguments and a\n",
      "                generation config. If a stopping criteria is passed that is already created with the arguments or a\n",
      "                generation config an error is thrown. If your stopping criteria depends on the `scores` input, make\n",
      "                sure you pass `return_dict_in_generate=True, output_scores=True` to `generate`. This feature is\n",
      "                intended for advanced users.\n",
      "            prefix_allowed_tokens_fn (`Callable[[int, torch.Tensor], List[int]]`, *optional*):\n",
      "                If provided, this function constraints the beam search to allowed tokens only at each step. If not\n",
      "                provided no constraint is applied. This function takes 2 arguments: the batch ID `batch_id` and\n",
      "                `input_ids`. It has to return a list with the allowed tokens for the next generation step conditioned\n",
      "                on the batch ID `batch_id` and the previously generated tokens `inputs_ids`. This argument is useful\n",
      "                for constrained generation conditioned on the prefix, as described in [Autoregressive Entity\n",
      "                Retrieval](https://arxiv.org/abs/2010.00904).\n",
      "            synced_gpus (`bool`, *optional*):\n",
      "                Whether to continue running the while loop until max_length. Unless overridden, this flag will be set\n",
      "                to `True` if using `FullyShardedDataParallel` or DeepSpeed ZeRO Stage 3 with multiple GPUs to avoid\n",
      "                deadlocking if one GPU finishes generating before other GPUs. Otherwise, defaults to `False`.\n",
      "            assistant_model (`PreTrainedModel`, *optional*):\n",
      "                An assistant model that can be used to accelerate generation. The assistant model must have the exact\n",
      "                same tokenizer. The acceleration is achieved when forecasting candidate tokens with the assistant model\n",
      "                is much faster than running generation with the model you're calling generate from. As such, the\n",
      "                assistant model should be much smaller.\n",
      "            streamer (`BaseStreamer`, *optional*):\n",
      "                Streamer object that will be used to stream the generated sequences. Generated tokens are passed\n",
      "                through `streamer.put(token_ids)` and the streamer is responsible for any further processing.\n",
      "            negative_prompt_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "                The negative prompt needed for some processors such as CFG. The batch size must match the input batch\n",
      "                size. This is an experimental feature, subject to breaking API changes in future versions.\n",
      "            negative_prompt_attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "                Attention_mask for `negative_prompt_ids`.\n",
      "            kwargs (`Dict[str, Any]`, *optional*):\n",
      "                Ad hoc parametrization of `generation_config` and/or additional model-specific kwargs that will be\n",
      "                forwarded to the `forward` function of the model. If the model is an encoder-decoder model, encoder\n",
      "                specific kwargs should not be prefixed and decoder specific kwargs should be prefixed with *decoder_*.\n",
      "\n",
      "        Return:\n",
      "            [`~utils.ModelOutput`] or `torch.LongTensor`: A [`~utils.ModelOutput`] (if `return_dict_in_generate=True`\n",
      "            or when `config.return_dict_in_generate=True`) or a `torch.LongTensor`.\n",
      "\n",
      "                If the model is *not* an encoder-decoder model (`model.config.is_encoder_decoder=False`), the possible\n",
      "                [`~utils.ModelOutput`] types are:\n",
      "\n",
      "                    - [`~generation.GenerateDecoderOnlyOutput`],\n",
      "                    - [`~generation.GenerateBeamDecoderOnlyOutput`]\n",
      "\n",
      "                If the model is an encoder-decoder model (`model.config.is_encoder_decoder=True`), the possible\n",
      "                [`~utils.ModelOutput`] types are:\n",
      "\n",
      "                    - [`~generation.GenerateEncoderDecoderOutput`],\n",
      "                    - [`~generation.GenerateBeamEncoderDecoderOutput`]\n",
      "        \n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
